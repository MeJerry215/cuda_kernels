import numpy as np

# torch.Size([1, 32, 128]) torch.Size([7297, 32, 128, 1]) torch.Size([7297, 32, 128])
# at_out = scale * torch.matmul(
#     query.reshape(1, query.shape[1], -1, query.shape[-1]),
#     key.reshape(key.shape[0], key.shape[1], key.shape[2], 1).transpose(0, 3),
# )
# # 1, 32, 1, 7297
# at_out = torch.softmax(at_out, dim=-1)
# o = torch.matmul(
#     at_out,
#     value.reshape(1, value.shape[0], value.shape[1], value.shape[2]).transpose(1, 2)
# )
# print(o.shape)
# print(torch.allclose(out.view(-1), o.view(-1), atol=1e-3, rtol=1e-3))

import torch

import math
import random
np.set_printoptions(precision=3, suppress=True)

torch.manual_seed(42)
np.random.seed(42)
random.seed(42)

# num_seqs = 2
# num_heads = 32
# num_kv_heads = 32  # æš‚æ—¶æ²¡ç”¨
# seq_len = 1
# head_size = 128
# context_lens = [23, 71]

num_seqs = 2
num_heads = 32
num_kv_heads = 32
seq_len = 1
head_size = 128
context_lens = [65, 13]


max_context_len = max(context_lens)
block_size = 16
FLT_MIN = np.finfo(np.float32).min
querys = torch.randn(num_seqs, num_heads, seq_len, head_size).float()
scale = float(head_size)**-0.5
x = 16 // 2  # è¿™é‡Œfp16 size 2  æ‰€ä»¥æ˜¯ä¸ª8 pack æˆ‘ä»¬è™½ç„¶æ˜¯ä½¿ç”¨float16 caseï¼Œä½†æ˜¯æ¨¡æ‹Ÿç”¨ä»»æ„æ•°æ®éƒ½è¡Œï¼Œè€Œä¸ºäº†ç†è§£åº•å±‚è®¡ç®—é€»è¾‘
NUM_BLOCKS = 64  # æ„æ€ä¸‹ è¿™ä¸ªæ˜¯å¯ä»¥GPU å†…å­˜å¯ä»¥åˆ†é…å‡ºæ¥çš„æœ€å¤§ kv cache blockå¤§å°
key_cache_shape = (NUM_BLOCKS, num_heads, head_size // x, block_size, x)
value_cache_shape = (NUM_BLOCKS, num_heads, head_size, block_size)
key_cache_np = np.zeros(key_cache_shape, dtype="float32")
value_cache_np = np.zeros(value_cache_shape, dtype="float32")
# V 1, 32, 6 + 1, 128
# QK^T = 1, 32, 1 128 @ 1, 32, 128, 7297  -> 1, 32, 1, 7297 -> softmax  1, 32, 1, 7297 -> @V  1, 32, 7297, 128  -> 1, 32, 1 128  -> transpose(1, 2)  -> 2, 6, 4096
# keys is transposed already, tranpose(1, 2)
keys = [torch.randn(num_heads, head_size, context_len).float()
        for context_len in context_lens]
values = [torch.randn(num_heads, context_len, head_size).float()
          for context_len in context_lens]


# values = [torch.tensor([i * 0.001 for i in range(num_heads * context_len * head_size)]).float().view(num_heads, context_len, head_size)
#           for context_len in context_lens]

querys_np = querys.numpy()
keys_np = [key.numpy() for key in keys]
values_np = [value.numpy() for value in values]


# å°† key value cacheè½¬æ¢ä¸º paged attentionçš„æ ·å¼
# KCache [num_blocks, num_kv_heads, head_size/x, block_size, x]
# VCache [num_blocks, num_kv_heads, head_size, block_size]
# paged_attention_v1 çš„å…¥å‚ä¸º output, query, key_cache, value_cache, num_kv_heads, scale, block_tables, context_lens, block_size, max_context_len, alibi_slopes, kv_cache_dtype,
# output,  num_kv_heads, block_tables
block_tables = []
max_num_blocks_per_seq = (max_context_len + block_size - 1) // block_size
for _ in range(num_seqs):
    block_table = [
        random.randint(0, NUM_BLOCKS - 1)
        for _ in range(max_num_blocks_per_seq)
    ]
    block_tables.append(block_table)  # block_tables è®°å½•çš„æ˜¯çœŸå®çš„blockåç§»

# ä¼ ç»Ÿçš„key value è½¬æ¢ä¸º key_cache, value_cache
# num_heads, head_size, context_len -> key_cache_shape = (NUM_BLOCKS, num_heads, head_size // x, block_size, x)
# num_heads, context_len, head_size -> value_cache_shape = (NUM_BLOCKS, num_heads, head_size, block_size)
#


# vllm ä¸­è°ƒç”¨opä¹‹å‰block_tables, å½“cache ä¸ç­‰çš„æ—¶å€™ä¼šèƒƒéƒ¨å¡«0 å¯¹é½
# tensor([[7384, 7380, 7378, 7376, 7374, 7372,    0],
#         [7383, 7382, 7381, 7379, 7377, 7375, 7373]], device='cuda:0',dtype=torch.int32
for i in range(len(block_tables)):
    block_table = block_tables[i]
    context_len = context_lens[i]
    key = keys_np[i]
    value = values_np[i]
    for context_idx in range(context_len):
        block_num = block_table[context_idx // block_size]
        block_off = context_idx % block_size
        key_slice = key[:, :, context_idx].reshape(
            num_heads, head_size // x, x)
        value_slice = value[:, context_idx, :]
        # print(key_cache_np[block_num, :, block_off, :].shape, key_slice.shape)
        # print(value_cache_np[block_num, :, :, block_off].shape, value_slice.shape)
        key_cache_np[block_num, :, :, block_off, :] = key_slice
        value_cache_np[block_num, :, :, block_off] = value_slice


def ref_single_query_kv_cache(querys, keys, values):
    attn_outs = []
    for seq_idx in range(num_seqs):
        query = querys[seq_idx]
        key = keys[seq_idx]
        value = values[seq_idx]
        attn_out = scale * torch.matmul(query, key)
        attn_out = torch.softmax(attn_out, dim=-1)
        attn_out = torch.matmul(attn_out, value)
        attn_outs.append(attn_out)
    return attn_outs


def ref_plain_paged_attention_v0(querys_np, keys_np, values_np,):
    attn_outs_np = []
    for seq_idx in range(num_seqs):
        query = querys_np[seq_idx]     # num_heads, 1, head_size
        key = keys_np[seq_idx]         # num_heads, head_size, context_len
        value = values_np[seq_idx]     # num_heads, context_len, head_size
        attn_out = np.zeros(
            (num_heads, seq_len, context_lens[seq_idx]), dtype="float32")          # qk^t out
        # è¿›è¡Œbatch gemv, Aå°‘äº†ä¸€ä¸ªç»´åº¦ï¼Œå› ä¸ºé»˜è®¤generate é˜¶æ®µå‡º 1 seq_len é•¿åº¦
        for num_head_idx in range(num_heads):
            for context_idx in range(context_lens[seq_idx]):
                sum = 0
                for head_size_idx in range(head_size):
                    sum += query[num_head_idx, 0, head_size_idx] * \
                        key[num_head_idx, head_size_idx, context_idx]
                sum *= scale
                attn_out[num_head_idx, 0, context_idx] = sum
        # print(np.allclose(attn_out, attn_outs[seq_idx], atol=1e-3, rtol=1e-3))
        # attn_out shape ( num_head, seq_len, context_len)
        # softmax æœ€åä¸€ä¸ªè½´ï¼ŒåŒæ—¶ä½¿ç”¨çš„safe softmax  å®ç°ä¸º reduce_max sub exp sum div
        for num_head_idx in range(num_heads):
            head_max = FLT_MIN
            # reduce max
            for context_idx in range(context_lens[seq_idx]):
                head_max = max(
                    head_max, attn_out[num_head_idx, 0, context_idx])
            # sub
            for context_idx in range(context_lens[seq_idx]):
                attn_out[num_head_idx, 0, context_idx] -= head_max
            # exp
            for context_idx in range(context_lens[seq_idx]):
                attn_out[num_head_idx, 0, context_idx] = math.exp(
                    attn_out[num_head_idx, 0, context_idx])
            # sum
            head_sum = 0
            for context_idx in range(context_lens[seq_idx]):
                head_sum += attn_out[num_head_idx, 0, context_idx]
            # div
            for context_idx in range(context_lens[seq_idx]):
                attn_out[num_head_idx, 0, context_idx] /= head_sum
        # batch gemm
        # print(np.allclose(attn_out, attn_outs[seq_idx], atol=1e-3, rtol=1e-3))
        # (num_heads, seq_len, context_len) @ (num_heads, context_len, head_size)
        attn_out_final = np.zeros(
            (num_heads, seq_len, head_size), dtype="float32")
        for num_head_idx in range(num_heads):
            for head_size_idx in range(head_size):
                sum = 0
                for context_idx in range(context_lens[seq_idx]):
                    sum += attn_out[num_head_idx, 0, context_idx] * \
                        value[num_head_idx, context_idx, head_size_idx]
                attn_out_final[num_head_idx, 0, head_size_idx] = sum
        # attn_outs_np[seq_idx] = attn_out_final
        attn_outs_np.append(attn_out_final)
        # print(np.allclose(attn_out_final, attn_outs[seq_idx], atol=1e-3, rtol=1e-3))
    return attn_outs_np


def ref_plain_paged_attention_v1(querys_np, key_cache_np, value_cache_np, block_tables, context_lens, max_context_len):
    attn_outs_np = []
    for seq_idx in range(num_seqs):
        attn_out = np.zeros(
            (num_heads, seq_len, context_lens[seq_idx]), dtype="float32")
        query = querys_np[seq_idx]
        block_table = block_tables[seq_idx]
        context_len = context_lens[seq_idx]
        for num_head_idx in range(num_heads):
            # batch gemv è®¡ç®—
            for context_idx in range(context_len):
                sum = 0
                block_num = block_table[context_idx // block_size]
                block_off = context_idx % block_size
                for head_sub_idx in range(head_size // x):
                    for x_idx in range(x):
                        sum += query[num_head_idx, 0, head_sub_idx * x + x_idx] * key_cache_np[block_num, num_head_idx, head_sub_idx, block_off, x_idx]
                sum *= scale
                attn_out[num_head_idx, 0, context_idx] = sum
        # softmax è®¡ç®—
        for num_head_idx in range(num_heads):
            head_max = FLT_MIN
            # reduce_max
            for context_idx in range(context_lens[seq_idx]):
                head_max = max(
                    head_max, attn_out[num_head_idx, 0, context_idx])
            # sub
            for context_idx in range(context_lens[seq_idx]):
                attn_out[num_head_idx, 0, context_idx] -= head_max
            # exp
            for context_idx in range(context_lens[seq_idx]):
                attn_out[num_head_idx, 0, context_idx] = math.exp(
                    attn_out[num_head_idx, 0, context_idx])
            # sum
            head_sum = 0
            for context_idx in range(context_lens[seq_idx]):
                head_sum += attn_out[num_head_idx, 0, context_idx]
            # div
            for context_idx in range(context_lens[seq_idx]):
                attn_out[num_head_idx, 0, context_idx] /= head_sum

        # attn_outs_np.append(attn_out)
        attn_out_final = np.zeros(
            (num_heads, seq_len, head_size), dtype="float32")
        for num_head_idx in range(num_heads):
            for head_size_idx in range(head_size):
                sum = 0
                for context_idx in range(context_lens[seq_idx]):
                    block_num = block_table[context_idx // block_size]
                    block_off = context_idx % block_size
                    sum += attn_out[num_head_idx, 0, context_idx] * \
                        value_cache_np[block_num, num_head_idx,
                                       head_size_idx, block_off]
                attn_out_final[num_head_idx, 0, head_size_idx] = sum
        attn_outs_np.append(attn_out_final)
    return attn_outs_np


# V2 ä¸»è¦æ˜¯ å°†å¾ªç¯åˆå¹¶æ‰, å¾ªç¯åœ¨è®¡ç®—è¿‡ç¨‹ä¸­ä¼šä¸€ç›´å ç”¨å¯„å­˜å™¨ å…±äº«å†…å­˜ç­‰èµ„æºï¼ŒåŒæ—¶åˆå¹¶å¾ªç¯ä¹Ÿèƒ½å¢åŠ ç®—æ•°å¼ºåº¦(åœ¨å¯„å­˜å™¨/å…±äº«å†…å­˜ä¸Š)
def ref_plain_paged_attention_v2(querys_np, key_cache_np, value_cache_np, block_tables, context_lens, max_context_len):
    # å‚è€ƒ https://zhuanlan.zhihu.com/p/673284781 vLLMçš‡å† ä¸Šçš„æ˜ç ï¼šæ·±å…¥æµ…å‡ºç†è§£PagedAttention CUDAå®ç° å¢åŠ éƒ¨åˆ†cuda kernel ç›¸å…³æ³¨è§£
    # paged attention çš„å®è´¨ æ˜¯block å»ç®— gemv softmax gemv è¿™æ ·çš„æ–¹å¼
    # NUM_THREADS é»˜è®¤ä¸º128, è¿™ä¸ªä¹Ÿå’Œhead_size ç›¸å…³, head_size å¸¸ä¸º128ï¼Œåˆ™1ä¸ªçº¿ç¨‹å®é™…æ˜¯ç®—ä¸€ä¸ªæ•°ï¼Œä¸”128çš„block åœ¨cuda ä¸­ä¹Ÿæ˜¯ç›¸å½“å¸¸è§
    # gridå¤§å°(num_heads, num_seqs) ä¸åŒgridä¹‹é—´æ˜¾ç„¶æ²¡æœ‰ä¾èµ–ï¼Œæ˜¯éå¸¸åˆç†çš„åˆ‡åˆ†
    # æ‰€ä»¥ ç”±äºattention çš„æœºåˆ¶åŸå› ï¼Œä¹Ÿæ˜¯å¯ä»¥ç†è§£ä¸º grid(num_heads * num_seqs) block(head_dim)
    # è¿™é‡Œè€ƒè™‘ä¸€ä¸‹ æ˜¯å¦å¯ä»¥å¼€æ›´å¤§çš„block ä»è€Œè®© block åšæ›´å¤šçš„äº‹æ¯”å¦‚ 256, åˆ™grid å¯ä»¥å‡åŠ å¢åŠ ä¸€äº›ï¼Œ num_head é€šå¸¸ä¸º32ç­‰ä¹‹ç±»çš„å¶æ•°ï¼Œæ‰€ä»¥å­˜åœ¨å¯èƒ½æ€§ï¼Œç”šè‡³åˆ°512
    # ä¸è¿‡ç”±äº cuda å¤šsm å¼€å°block æ›´å…·æœ‰æ€§ä»·æ¯” 256 çš„å®ç°ä¹Ÿè®¸å¯ä»¥è¯•è¯•
    # x = 8 ä¸º sizeof(half) * x = 16 bytes = 128bit è¿™ä¸ªå¯ä»¥ä½¿ç”¨LDG.E.128 ä¸€æ¬¡åŠ è½½ä¸Šæ¥8ä¸ªæ•°
    # æ–‡ä¸­è¯´ 128 çš„çº¿ç¨‹å— åˆ†ä¸º 4 warps, è€Œæ¯ä¸ªwarp ä¹Ÿåˆ†æˆäº†blk_size çš„thread_group
    attn_outs_np = []
    for seq_idx in range(num_seqs):
        query = querys_np[seq_idx]
        block_table = block_tables[seq_idx]
        context_len = context_lens[seq_idx]
        attn_out_final = np.zeros(
            (num_heads, seq_len, head_size), dtype="float32")
        for num_head_idx in range(num_heads):
            # è°ƒæ•´äº†attn_out çš„ä½ç½®, å¦‚æœè¿™éƒ¨åˆ†å ç”¨èµ„æº, ç›¸æ¯”åŸå…ˆä¸ºåŸæ¥çš„ 1 / num_heads
            # å…±äº«å†…å­˜å ç”¨ max_context_len * sizeof(float16)
            attn_out = np.zeros(
                (seq_len, context_lens[seq_idx]), dtype="float32")
            # batch gemv è®¡ç®—
            head_max = FLT_MIN
            for context_idx in range(context_len):
                sum = 0
                block_num = block_table[context_idx // block_size]
                block_off = context_idx % block_size
                for head_sub_idx in range(head_size // x):
                    for x_idx in range(x):
                        sum += query[num_head_idx, 0, head_sub_idx * x + x_idx] * \
                            key_cache_np[block_num, num_head_idx,
                                         head_sub_idx, block_off, x_idx]
                sum *= scale
                head_max = max(head_max, sum)
                attn_out[0, context_idx] = sum
            # åœ¨è®¡ç®—softmax è¿™é‡Œç”±äºä½¿ç”¨çš„æ˜¯safe softmaxçš„ç®—æ³•ï¼Œæ‰€ä»¥ä¸¤æ¬¡reduce block å†…å¿…é¡»åŒæ­¥ï¼Œæ‰€ä»¥è¯´æ€§èƒ½ä¸Šä¼šæ…¢ä¸€ç‚¹ï¼Œ
            # sub exp sum è¿™é‡Œä¾èµ– head_max å¿…é¡»è¦å…ˆç®—å®Œ æ‰èƒ½ç®—æ•´ä½“çš„, flashattentionçš„æ”¹è¿› å¯ä»¥ç®—online softmax è¿™é‡Œå°±ä¸ç”¨ç­‰äº†ï¼Œä¸€ä¸ªéå¸¸ä¸é”™çš„ä¼˜åŒ–ç‚¹
            head_sum = 0
            for context_idx in range(context_lens[seq_idx]):
                exp_val = math.exp(attn_out[0, context_idx] - head_max)
                attn_out[0, context_idx] = exp_val
                head_sum += exp_val

            for head_size_idx in range(head_size):
                sum = 0
                for context_idx in range(context_lens[seq_idx]):
                    block_num = block_table[context_idx // block_size]
                    block_off = context_idx % block_size
                    sum += attn_out[0, context_idx] * value_cache_np[block_num,
                                                                     num_head_idx, head_size_idx, block_off] / head_sum
                attn_out_final[num_head_idx, 0, head_size_idx] = sum
        attn_outs_np.append(attn_out_final)
    return attn_outs_np


def ref_plain_paged_attention_v3(querys_np, key_cache_np, value_cache_np, block_tables, context_lens, max_context_len):
    # 2-pass
    # https://zhuanlan.zhihu.com/p/663932651
    # where N is sequence length  d is head_size  inference N = 1
    # torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 128, 23]) torch.Size([1, 32, 23, 128])
    #                   torch.Size([1, 32, 1, 23])         torch.Size([1, 32, 23, 128])
    #                                       torch.Size([1, 32, 1, 128])
    # flash attention çš„å®ç° online softmax å‡å°‘è®¿å­˜ å†…å­˜è®¿é—®ç©ºé—´å¤æ‚åº¦ s ğ‘‚(ğ‘^2ğ‘‘^2ğ‘€^âˆ’1)
    # 1. è®¡ç®—softmax çš„reductionçš„æ—¶å€™æ²¡æœ‰å¿…è¦è®¿é—®æ‰€æœ‰çš„è¾“å…¥
    # 2. åå‘çš„æ—¶å€™ä¸å­˜å‚¨ä¸­é—´ç»“æœçŸ©é˜µ
    # å¸¸è§„ç®—æ³•ä¸­safe softmax çš„reduceè½´ æ˜¯ä¸å¯åˆ†çš„ï¼Œonline softmaxåˆ™æ˜¯å°†reduce åˆ†æˆå¤šä¸ªå—ï¼Œ
    # paged attentionçš„ä¸»è¦é—®é¢˜æ˜¯ï¼šå½“ num_heads * num_seqs å°äºsmçš„ä¸ªæ•°çš„æ—¶å€™ï¼Œæ•´ä½“çš„å¹¶è¡Œåº¦æ˜¯ä¸å¤Ÿçš„
    # æ‰€ä»¥æ–‡ä¸­æå‡ºå°†è¾“å…¥åˆ†æˆå¤šä¸ªblock ä»¥åŠ é€šè¿‡tiling æé«˜softmax çš„æ€§èƒ½ tiling, recomputation
    # softmax åŸæœ¬çš„è®¡ç®—æ˜¯è¦æœ€åä¸€ç»´åº¦æ‰€æœ‰çš„å€¼ï¼Œç°åœ¨å°†æœ€åä¸€ç»´è¿›è¡Œåˆ†å—è®¡ç®—ï¼Œ
    attn_outs_np = []
    for seq_idx in range(num_seqs):
        query = querys_np[seq_idx]
        block_table = block_tables[seq_idx]
        context_len = context_lens[seq_idx]
        attn_out_final = np.zeros(
            (num_heads, seq_len, head_size), dtype="float32")
        for num_head_idx in range(num_heads):
            # è°ƒæ•´äº†attn_out çš„ä½ç½®, å¦‚æœè¿™éƒ¨åˆ†å ç”¨èµ„æº, ç›¸æ¯”åŸå…ˆä¸ºåŸæ¥çš„ 1 / num_heads
            # å…±äº«å†…å­˜å ç”¨ max_context_len * sizeof(float16)
            attn_out = np.zeros(
                (seq_len, context_lens[seq_idx]), dtype="float32")
            # batch gemv è®¡ç®—
            head_max = FLT_MIN
            head_sum = 0
            for context_idx in range(context_len):
                pre_max = head_max
                sum = 0
                block_num = block_table[context_idx // block_size]
                block_off = context_idx % block_size
                # online softmax
                for head_sub_idx in range(head_size // x):
                    for x_idx in range(x):
                        sum += query[num_head_idx, 0, head_sub_idx * x + x_idx] * \
                            key_cache_np[block_num, num_head_idx,
                                         head_sub_idx, block_off, x_idx]
                sum *= scale
                head_max = max(head_max, sum)
                attn_out[0, context_idx] = sum
                head_sum = head_sum * \
                    math.exp((pre_max - head_max)) + math.exp(sum - head_max)
            # head_sums = np.zeros((head_size), dtype="float32")
            for head_size_idx in range(head_size):
                sum = 0
                for context_idx in range(context_len):
                    block_num = block_table[context_idx // block_size]
                    block_off = context_idx % block_size
                    # recompute
                    sum += math.exp(attn_out[0, context_idx] - head_max) * \
                        value_cache_np[block_num, num_head_idx,
                                       head_size_idx, block_off] / head_sum
                attn_out_final[num_head_idx, 0, head_size_idx] += sum
        attn_outs_np.append(attn_out_final)
    return attn_outs_np


def ref_plain_paged_attention_v4(querys_np, key_cache_np, value_cache_np, block_tables, context_lens, max_context_len):
    # one-pass paged attentio1n
    # online softmax å¼•å…¥ç®€åŒ–paged attentionåˆ°å¦‚ä¸‹å½¢å¼
    # https://zhuanlan.zhihu.com/p/663932651
    attn_outs_np = []
    for seq_idx in range(num_seqs):
        query = querys_np[seq_idx]
        block_table = block_tables[seq_idx]
        context_len = context_lens[seq_idx]
        attn_out = np.zeros(
            (num_heads, seq_len, head_size), dtype="float32")
        for num_head_idx in range(num_heads):
            head_max = FLT_MIN
            head_sum = 0
            sum_values = np.zeros((head_size,), dtype="float32")
            # outer loop
            for context_idx in range(context_len):  # i/N
                # m_{i-1}
                pre_max = head_max
                # d_{i-1}
                pre_sum = head_sum
                qk_sum = 0
                block_num = block_table[context_idx // block_size]
                block_off = context_idx % block_size
                # inter loop è¿™é‡Œquery ç»´åº¦seq_len ä¸º1ï¼Œå¯¹äºprefillé˜¶æ®µ è¿™é‡Œå°±æ˜¯æ–¹é˜µ
                # Q (1, 32, N, 128)  K^T (1, 32, 128, N) V (1, 32, N, 128)
                #       QK^T (1, 32, N, N)   V (1, 32, N, 128)
                #                (1, 32, N, 128)
                for head_sub_idx in range(head_size // x):
                    for x_idx in range(x):
                        qk_sum += query[num_head_idx, 0, head_sub_idx * x + x_idx] * \
                            key_cache_np[block_num, num_head_idx,
                                         head_sub_idx, block_off, x_idx]
                # x_i
                qk_sum *= scale
                # print("qk_sum4", qk_sum)
                # m_i
                head_max = max(head_max, qk_sum)
                head_sum = pre_sum * \
                    math.exp((pre_max - head_max)) + \
                    math.exp(qk_sum - head_max)  # d_i
                for head_size_idx in range(head_size):
                    sum_values[head_size_idx] = sum_values[head_size_idx] * \
                        pre_sum * math.exp(pre_max - head_max) / head_sum
                # print("sum_values4", sum_values)
                # print("qk_sums4", qk_sums)
                # print(f"before {context_idx} sum_values4", sum_values, "head_sum", head_sum, "qk_sum", qk_sum, "head_max", head_max)
                for head_size_idx in range(head_size):
                    sum_values[head_size_idx] += math.exp(
                        qk_sum - head_max) * value_cache_np[block_num, num_head_idx, head_size_idx, block_off] / head_sum
                # print(f"after {context_idx} sum_values4", sum_values, "head_sum", head_sum, "qk_sum", qk_sum, "head_max", head_max)
                # print("sum_values4",sum_values)
            attn_out[num_head_idx, 0, :] = sum_values
        attn_outs_np.append(attn_out)
    return attn_outs_np


def ref_plain_paged_attention_v5(querys_np, key_cache_np, value_cache_np, block_tables, context_lens, max_context_len):
    # https://zhuanlan.zhihu.com/p/663932651
    # vllm ä¸­1ä¸ªwarp ç®—1ä¸ªblock(16 context len)  1 warp 1 blcok   ä¸€ä¸ªblock 128 thread ä¸º4warp æ‰€ä»¥subcontext_size = 64
    # flash attention è¿›è¡Œäº†tiling å¦‚æœtiling æ‰€ä»¥subcontext_size = 64
    # flash attention æœ€ç»ˆå®ç° å¢åŠ äº†tiling
    # flash attention1 implementation
    attn_outs_np = []
    subcontext_size = 64
    for seq_idx in range(num_seqs):
        query = querys_np[seq_idx]
        block_table = block_tables[seq_idx]
        context_len = context_lens[seq_idx]
        attn_out = np.zeros(
            (num_heads, seq_len, head_size), dtype="float32")
        subcontext_num = (context_len + subcontext_size - 1) // subcontext_size
        for num_head_idx in range(num_heads):
            head_max = FLT_MIN      # è®°å½•context_lenå†…å½“å‰è¿­ä»£è¿‡ç¨‹ä¸­çš„maxå€¼ï¼Œä¹Ÿå¯ä»¥ç†è§£ä¸º m_{i-1}
            # head sum ä¹Ÿå¯ä»¥ç†è§£ä¸º subcontext d_{i-1} è®°å½•çš„æ˜¯ä¸Šæ¬¡è¿­ä»£çš„æ—¶å€™sumå‡ºæ¥çš„å€¼
            head_sum = 0
            sum_values = np.zeros((head_size,), dtype="float32")
            for subcontext_num_idx in range(subcontext_num):    # 1, 2
                qk_sums = np.full((subcontext_size, ), FLT_MIN, dtype="float32")

                for subcontext_idx in range(subcontext_size):   # 1-64
                    context_idx = subcontext_num_idx * subcontext_size + subcontext_idx
                    if context_idx >= context_len:
                        break
                    qk_sum = 0
                    block_num = block_table[context_idx // block_size]
                    block_off = context_idx % block_size
                    for head_sub_idx in range(head_size // x):
                        for x_idx in range(x):
                            qk_sum += query[num_head_idx, 0, head_sub_idx * x + x_idx] * \
                                key_cache_np[block_num, num_head_idx,
                                             head_sub_idx, block_off, x_idx]

                    qk_sum *= scale
                    # x_i = Q[k, :]K^T[:,ib: ib + b]
                    qk_sums[subcontext_idx] = qk_sum  # x_i i from 0 to 63
                subcontext_pre_max = head_max
                subcontext_pre_sum = head_sum
                subcontext_max = FLT_MIN
                # m_i^{local} = max(x_i) i from 0 to 63$
                for subcontext_idx in range(subcontext_size):
                    context_idx = subcontext_num_idx * subcontext_size + subcontext_idx
                    if context_idx >= context_len:
                        break
                    subcontext_max = max(subcontext_max, qk_sums[subcontext_idx])
                # m_i = max(m_{i-1}, m_i^{local})
                head_max = max(head_max, subcontext_max)
                # d_i = d_{i-1} e^{m_{i-1}-m_i}
                subcontext_sum = subcontext_pre_sum * \
                    math.exp(subcontext_pre_max - head_max)
                # d_i += \sum_{j=0}^{b} {e^{x_i[j] - m_i}}
                for subcontext_idx in range(subcontext_size):
                    context_idx = subcontext_num_idx * subcontext_size + subcontext_idx
                    if context_idx >= context_len:
                        break
                    subcontext_sum += math.exp(
                        qk_sums[subcontext_idx] - head_max)
                head_sum = subcontext_sum  # d_i
                # sum_values[head_size_idx] è¿­ä»£è¿‡ç¨‹ä¸­ä¸Šæ¬¡ç»“æœè¾“å‡º é¦–å…ˆç¬¬ä¸€æ­¥éœ€è¦å°†ä¸Šæ¬¡çš„ç»“æœä¿®æˆä¸ºå½“å‰maxå€¼çš„ç»“æœ
                # o_i = o_{i-1} \frac {d_{i-1}e^{m_{i-1}-m_i}} {d_i}
                # å¦‚æœm_{i-1} - m_i = 0 åˆ™æœ¬è½®æ²¡æœ‰æ”¹æœ€å¤§å€¼ï¼Œå‰é¢çš„ç»“æœä¹Ÿæ˜¯æŒ‰ç…§head_maxç´¯åŠ çš„ å¯ä»¥ç›´æ¥åŠ 
                # print(qk_sums)
                # 1 2  1 2 128
                for head_size_idx in range(head_size):
                    sum_values[head_size_idx] = sum_values[head_size_idx] * \
                        subcontext_pre_sum * \
                        math.exp(subcontext_pre_max - head_max) / head_sum
                for subcontext_idx in range(subcontext_size):
                    context_idx = subcontext_num_idx * subcontext_size + subcontext_idx
                    if context_idx >= context_len:
                        break
                    block_num = block_table[context_idx // block_size]
                    block_off = context_idx % block_size
                    # print(f"before {subcontext_idx} sum_values5", sum_values, "head_sum", head_sum, "qk_sum", qk_sums[subcontext_idx], "head_max", head_max)
                    # print(subcontext_idx, qk_sums[subcontext_idx], math.exp(qk_sums[subcontext_idx] - head_max))
                    for head_size_idx in range(head_size):
                        sum_values[head_size_idx] += math.exp(qk_sums[subcontext_idx] - head_max) * \
                            value_cache_np[block_num, num_head_idx,
                                           head_size_idx, block_off] / head_sum
                    # print(f"after {subcontext_idx} sum_values5", sum_values, "head_sum", head_sum, "qk_sum", qk_sums[subcontext_idx], "head_max", head_max)
                    # print("sum_values5",sum_values)
            attn_out[num_head_idx, 0, :] = sum_values
        attn_outs_np.append(attn_out)
    return attn_outs_np


def ref_plain_paged_attention_v6(querys_np, key_cache_np, value_cache_np, block_tables, context_lens, max_context_len):
    # flash attention v2 ç›¸æ¯”attention v1 åŠ é€Ÿäº†2x ä¸»è¦æ”¹è¿›ï¼š
    # 1. å‡å°‘éä¹˜æ³•è®¡ç®—
    # 2. ä¼˜åŒ–qkv for å¾ªç¯é¡ºåº
    # 3. é‡‡ç”¨shared memory å‡å°‘é€šä¿¡
    attn_outs_np = []
    subcontext_size = 64
    for seq_idx in range(num_seqs):
        query = querys_np[seq_idx]
        block_table = block_tables[seq_idx]
        context_len = context_lens[seq_idx]
        attn_out = np.zeros(
            (num_heads, seq_len, head_size), dtype="float32")
        subcontext_num = (context_len + subcontext_size - 1) // subcontext_size
        for num_head_idx in range(num_heads):
            head_max = FLT_MIN      # è®°å½•context_lenå†…å½“å‰è¿­ä»£è¿‡ç¨‹ä¸­çš„maxå€¼ï¼Œä¹Ÿå¯ä»¥ç†è§£ä¸º m_{i-1}
            # head sum ä¹Ÿå¯ä»¥ç†è§£ä¸º subcontext d_{i-1} è®°å½•çš„æ˜¯ä¸Šæ¬¡è¿­ä»£çš„æ—¶å€™sumå‡ºæ¥çš„å€¼
            head_sum = 0
            sum_values = np.zeros((head_size,), dtype="float32")
            for subcontext_num_idx in range(subcontext_num):    # 1, 2
                qk_sums = np.full((subcontext_size, ), FLT_MIN, dtype="float32")

                for subcontext_idx in range(subcontext_size):   # 1-64
                    context_idx = subcontext_num_idx * subcontext_size + subcontext_idx
                    if context_idx >= context_len:
                        break
                    qk_sum = 0
                    block_num = block_table[context_idx // block_size]
                    block_off = context_idx % block_size
                    for head_sub_idx in range(head_size // x):
                        for x_idx in range(x):
                            qk_sum += query[num_head_idx, 0, head_sub_idx * x + x_idx] * \
                                key_cache_np[block_num, num_head_idx,
                                             head_sub_idx, block_off, x_idx]

                    qk_sum *= scale
                    # x_i = Q[k, :]K^T[:,ib: ib + b]
                    qk_sums[subcontext_idx] = qk_sum  # x_i i from 0 to 63
                subcontext_pre_max = head_max
                subcontext_pre_sum = head_sum
                subcontext_max = FLT_MIN
                # m_i^{local} = max(x_i) i from 0 to 63$
                for subcontext_idx in range(subcontext_size):
                    context_idx = subcontext_num_idx * subcontext_size + subcontext_idx
                    if context_idx >= context_len:
                        break
                    subcontext_max = max(subcontext_max, qk_sums[subcontext_idx])
                # m_i = max(m_{i-1}, m_i^{local})
                head_max = max(head_max, subcontext_max)
                # d_i = d_{i-1} e^{m_{i-1}-m_i}
                subcontext_sum = subcontext_pre_sum * \
                    math.exp(subcontext_pre_max - head_max)
                # d_i += \sum_{j=0}^{b} {e^{x_i[j] - m_i}}
                for subcontext_idx in range(subcontext_size):
                    context_idx = subcontext_num_idx * subcontext_size + subcontext_idx
                    if context_idx >= context_len:
                        break
                    subcontext_sum += math.exp(
                        qk_sums[subcontext_idx] - head_max)
                head_sum = subcontext_sum  # d_i
                # sum_values[head_size_idx] è¿­ä»£è¿‡ç¨‹ä¸­ä¸Šæ¬¡ç»“æœè¾“å‡º é¦–å…ˆç¬¬ä¸€æ­¥éœ€è¦å°†ä¸Šæ¬¡çš„ç»“æœä¿®æˆä¸ºå½“å‰maxå€¼çš„ç»“æœ
                # o_i = o_{i-1} \frac {d_{i-1}e^{m_{i-1}-m_i}} {d_i}
                # å¦‚æœm_{i-1} - m_i = 0 åˆ™æœ¬è½®æ²¡æœ‰æ”¹æœ€å¤§å€¼ï¼Œå‰é¢çš„ç»“æœä¹Ÿæ˜¯æŒ‰ç…§head_maxç´¯åŠ çš„ å¯ä»¥ç›´æ¥åŠ 
                # print(qk_sums)
                # 1 2  1 2 128
                for head_size_idx in range(head_size):
                    sum_values[head_size_idx] = sum_values[head_size_idx] * \
                        subcontext_pre_sum * \
                        math.exp(subcontext_pre_max - head_max) / head_sum
                for subcontext_idx in range(subcontext_size):
                    context_idx = subcontext_num_idx * subcontext_size + subcontext_idx
                    if context_idx >= context_len:
                        break
                    block_num = block_table[context_idx // block_size]
                    block_off = context_idx % block_size
                    # print(f"before {subcontext_idx} sum_values5", sum_values, "head_sum", head_sum, "qk_sum", qk_sums[subcontext_idx], "head_max", head_max)
                    # print(subcontext_idx, qk_sums[subcontext_idx], math.exp(qk_sums[subcontext_idx] - head_max))
                    for head_size_idx in range(head_size):
                        sum_values[head_size_idx] += math.exp(qk_sums[subcontext_idx] - head_max) * \
                            value_cache_np[block_num, num_head_idx,
                                           head_size_idx, block_off] / head_sum
                    # print(f"after {subcontext_idx} sum_values5", sum_values, "head_sum", head_sum, "qk_sum", qk_sums[subcontext_idx], "head_max", head_max)
                    # print("sum_values5",sum_values)
            attn_out[num_head_idx, 0, :] = sum_values
        attn_outs_np.append(attn_out)
    return attn_outs_np

# print(querys)
print(querys.shape, keys[0].shape, values[0].shape)
print(querys_np.shape, key_cache_np.shape, value_cache_np.shape, )
# attn_outs = ref_single_query_kv_cache(querys, keys, values)
# attn_outs_np_v0 = ref_plain_paged_attention_v0(querys_np, keys_np, values_np)
# attn_outs_np_v1 = ref_plain_paged_attention_v1(
#     querys_np, key_cache_np, value_cache_np, block_tables, context_lens, max_context_len)
# attn_outs_np_v2 = ref_plain_paged_attention_v2(
#     querys_np, key_cache_np, value_cache_np, block_tables, context_lens, max_context_len)
# attn_outs_np_v3 = ref_plain_paged_attention_v3(
#     querys_np, key_cache_np, value_cache_np, block_tables, context_lens, max_context_len)
attn_outs_np_v4 = ref_plain_paged_attention_v4(
    querys_np, key_cache_np, value_cache_np, block_tables, context_lens, max_context_len)
# attn_outs_np_v5 = ref_plain_paged_attention_v5(
#     querys_np, key_cache_np, value_cache_np, block_tables, context_lens, max_context_len)
attn_outs_np_v6 = ref_plain_paged_attention_v6(
    querys_np, key_cache_np, value_cache_np, block_tables, context_lens, max_context_len)

for num_seq in range(num_seqs):
    print(np.allclose(attn_outs_np_v4[num_seq],
          attn_outs_np_v6[num_seq], atol=1e-3, rtol=1e-3))

# print(attn_outs_np_v4[num_seq], attn_outs_np_v5[num_seq])

# é€å…¥åˆ°opsçš„ kv cache æ˜¯åˆ·çš„0 ä¸”æ˜¯å¯¹é½çš„,
# x = 16 // torch.tensor([], dtype=torch_dtype).element_size()

# key_caches, value_caches = kv_cache_factory(NUM_BLOCKS, block_size, 1,
#                                                 num_kv_heads, head_size,
#                                                 kv_cache_dtype, dtype, seed,
#                                                 device)
